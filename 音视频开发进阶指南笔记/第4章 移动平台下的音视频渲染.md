# 音频渲染

## AudioUnit

在iOS平台上，所有的音频框架底层都是基于AudioUnit实现的

![image.png](https://s2.loli.net/2022/12/30/U8jQyqM6I7d1eKu.png)

AudioUnit适合于以下场景：

+ 使用低延迟的音频I/O
+ 多路声音的合成并且回放
+ 使用AudioUnit里面提供的特有功能
+ 需要图状结构来处理音频、可以将音频处理模块组装到灵活的图状结构中。

### AudioSession

用于管理与获取iOS设备音频的硬件信息，并且是以单例的形式存在。

获取到单例之后，需要进行一些基本的设置：

+ 根据我们需要硬件设备提供的能力来设置类别
+ 设置I/O的Buffer，Buffer越小，则说明延迟越低
+ 设置采样频率，让硬件设备按照设置的采样频率来采集或者播放音频。
+ 激活AudioSession：[audioSession setActive:YES error:&error];

### 构建AudioUnit

构建AudioUnit的时候需要指定：类型、子类型、厂商。 厂商一般是固定的kAudioUnitManufacturer_Apple。

创建AudioUnit之前，首先创建AudioUnit的描述结构体

```
AudioComponentDescription ioUnitDescription; ioUnitDescription.componentType = kAudioUnitType_Output; ioUnitDescription.componentSubType = kAudioUnitSubType_RemoteIO; ioUnitDescription.componentManufacturer=kAudioUnitManufacturer_Apple; ioUnitDescription.componentFlags = 0; ioUnitDescription.componentFlagsMask = 0;
```

然后利用描述的结构体，创建AudioUnit，有两种方式：

+ 直接使用AudioUnit裸的创建方式

  ```
  首先根据AudioUnit的描述，找出实际的AudioUnit类型
  AudioComponent ioUnitRef = AudioComponentFindNext(NULL, &ioUnitDescription);
  
  然后声明一个AudioUnit引用
  AudioUnit ioUnitInstance;
  
  最后根据类型创建处这个AudioUnit实例：
  AudioComponentInstanceNew(ioUnitRef, &ioUnitInstance);
  ```

  

+ 使用AUGraph和AUNode。AUNode就是对AudioUnit的封装，可以理解为一个AudioUnit的wrapper

```
首先声明并实例化一个AUGraph
AUGraph processingGraph; 
NewAUGraph (&processingGraph);

然后按照AudioUnit的描述，在AUGraph中增加一个AUNode：
AUNode ioNode; 
AUGraphAddNode (processingGraph, &ioUnitDescription, &ioNode)

接下来打开AUGraph，这个过程是在实例化AUGraph中所有的AUNode。
AUGraphOpen(processingGraph)

最后在AUGraph中的某个Node里获得AudioUnit的引用
AudioUnit ioUnit;
AUGraphNodeInfo(processingGraph, ioNode, NULL,&ioUnit);
```

因为AUNode像是AudioUnit的一个封装，我们可以想象在AUNode中有一个成员属性，是AudioUnit类型的。通过使用AudioUnit的描述，我们创建出AUNode，从而可以通过这个node，来获得AudioUnit.

## AudioUnit的通用参数设置

首先需要将AudioUnit的输入输出端进行连接。以RemoteIO这个AudioUnit为例

![image.png](https://s2.loli.net/2022/12/30/kXvx4pAoPHq78UE.png)

RemoteIO Unit分为Element0和Element1，其中Element0控制输出端，Element1控制输入端，同时每个Element又分为Input Scope和 Output Scope。如果开发者想要使用扬声器的声音播放功能，那么必须将这个Unit的Element0的OutputScope和Speaker进行连接。而如果开发者想要使用麦克风的录音功能，那么必须将这个Unit的Element1的InputScope和麦克风进行连接。

### AudioStream的描述以及参数

```
UInt32 bytesPerSample = sizeof(Float32); 
AudioStreamBasicDescription asbd; 
bzero(&asbd, sizeof(asbd));
asbd.mFormatID = kAudioFormatLinearPCM; 
asbd.mSampleRate = _sampleRate; 
asbd.mChannelsPerFrame = channels; 
asbd.mFramesPerPacket = 1; 
asbd.mFormatFlags = kAudioFormatFlagsNativeFloatPacked | kAudioFormatFlagIsNonInterleaved; 
asbd.mBitsPerChannel = 8 * bytesPerSample; 
asbd.mBytesPerFrame = bytesPerSample; 
asbd.mBytesPerPacket = bytesPerSample;
```

## AudioUnit的分类

### EffectUnit

类型是kAudioUnitType_Effect。主要提供声音特效处理的功能。 子类型及用途如下：

+ 均衡效果器。子类型是kAudioUnitSubType_NBandEQ。主要作用是为声音的某些频带增强或者减弱能量。
+ 压缩效果器。子类型是kAudioUnitSubType_DynamicsProcessor。主要作用是声音较小的时候，可以提高声音的能量；当声音的能量超过设置的阈值时，可以降低声音的能量。
+ 混响效果器。子类型是kAudioUnitSubType_Reverb2。

### MixerUnit

类型是kAudioUnitType_Mixer，主要提供Mix多路声音的功能。子类型及用途如下：

+ 3D Mixer
+ ·MultiChannelMixer：子类型是 kAudioUnitSubType_MultiChannelMixer。是多路声音混音的效果器。

### I/OUnit

类型是kAudioUnitType_Output。提供I/O功能。

+ RemoteIO。子类型是kAudioUnitSubType_RemoteIO。用来采集音频和播放音频。
+ Generic Output。子类型是kAudioUnitSubType_GenericOutput。当在AUGraph中不使用speaker来驱动整个数据流，而是希望使用一个输出来驱动数据流时，就可以使用该类型。具体说，RemoteIO，output端连接的是speaker。而我们并不想要采集的声音直接输出到麦克风，而是连接到磁盘，储存起来，就用Generic Output。

### Format Converter Unit

类型是kAudioUnitType_FormatConverter，主要用于提供格式转换的功能

+ AUConverter。子类型是kAudioUnitSubType_AUConverter。格式转换效果器。
+ Time Pitch。子类型是kAudioUnitSubType_NewTimePitch。变调效果器。

### Generator Unit

类型是kAudioUnitType_Generator. 用来提供播放器的功能。

+ AudioFilePlayer。子类型是是kAudioUnitSubType_AudioFilePlayer。

## 构造AUGraph

不同的AudioNode有不同的作用，他们共同在AUgraph中协同发挥作用。那么如何将不同的AUNode连接起来？两种方式：

![image.png](https://s2.loli.net/2022/12/30/9rbEQ7VLWx5hkOY.png)

+ 直接将AUNode连接起来

  ```
  AUGraphConnectNodeInput(mPlayerGraph, mPlayerNode, 0, mPlayerIONOde, 0);
  这行代码作用是，将AudioFilePlayer Unit和RemoteIO Unit直接连接起来，当RemoteIO Unit需要播放数据的时候，就会调用AudioFilePlayer Unit来获取数据，这样就把这两个Audio Unit连接起来了。
  ```

  

+ 通过回调方式将两个AUNode连接起来。

```
AURenderCallBackStruct renderProc;
renderProc.inputProc = &inputAvailableCallBack;
renderProc.inputProcRefCon = (_bridge void *)self;
AUGraphSetNodeInputCallBack(mGraph, ioNode, 0, &renderProc);

//这段代码首先是构造一个AURenderCallback的结构体，并指定一个 回调函数，然后设置给RemoteIO Unit的输入端，当RemoteIO Unit需要 数据输入的时候就会回调该回调函数

```

# 视频渲染

## OpenGL渲染管线 (pipeline)

所谓的渲染管线，其实就是渲染流水线，渲染流程的意思。分为以下阶段

### 指定几何对象

主要决定如何去绘制几何图元。三种方式：

+ GL_POINTS:以点的形式进行绘制，通常用在绘制例子效果的场景中。
+ GL_LINES:以线的形式进行绘制，通常用在绘制直线的场景中。
+ GL_TRIANGLE_STRIP:以三角形的形式进行绘制，所有二维图像的渲染都使用这种方式。

### 顶点处理

这个阶段所做的操作是，根据模型视图和投影矩阵进行变换来改变顶点的位置；根据纹理坐标与纹理矩阵来改变纹理坐标的位置。如果涉及三维的渲染，处理光照计算和法线变换。

### 图元组装

将纹理组装成图元。

### 栅格化操作

将图元分解成更小的单元并对应于帧缓冲区的各个像素。这些单元称为片元，一个片元可能包含出窗口亚瑟、纹理坐标等属性。片元的属性是根据顶点坐标利用插值来确定的。

### 片元处理

根据纹理坐标取得纹理中对应的片元像素值，根据自己的业务处理来变换这个片元的颜色。

### 帧缓冲操作

主要执行帧缓冲的写入操作，负责将最终的像素值写到帧缓冲区中。

OpenGL ES 2.0版本与之前版本相比，提供了可编程的着色器来代替OpenGL ES中渲染管线的某一阶段。

Vertex Shader：顶点着色器，用来替换顶点处理阶段。

Fragment Shader：片元着色器，用来替换片元处理阶段。

## 创建shader

<img src="https://s2.loli.net/2022/12/30/Q48brZHaPMOKwCA.png" alt="image.png" style="zoom:33%;" />

### 调用glCreateShader方法创建一个对象，作为shader的容器，该函数会返回一个容器的句柄

```
GLuint glCreateShader（GLenum shaderType）;
```

### 为创建的shader添加源代码

即图中的两个shader content。

```
void glShaderSouce(GLuint shader, int numOfStrings, const char **strings, int *lenOfStrings);
```

### 编译shader

```
void glCompileShader(GLuint shader);
```

## 创建显卡可执行程序

### 首先创建一个对象，作为程序的容器，此函数将返回容器的句柄

```
GLuint glCreateProgram(void);
```

### 然后把编译的shader附加到刚刚创建的程序中

```
void glAttachShader(GLuint program, GLuint shader);
```

### 链接程序

```
void glLinkProgram(GLuint program);
```

### 使用程序

```
void glUseProgram(GLuint program);
```

## 上下文环境搭建

> OpenGL不负责窗口管理及上下文环境管理，该职责将由各平台或者设备自行完成。为了在OpenGL的输出与设备的屏幕之间架接起一个桥梁，iOS提供了EAGL。在iOS平台上，不允许开发者使用OpenGL ES直接渲染屏幕，必须使用FrameBuffer与RenderBuffer来进行渲染。若要使用EAGL，则必须先创建一个RenderBuffer，然后让OpenGL ES渲染到该renderBuffer上去。而该renderBuffer则需要绑定到一个CAEAGLLayer上面去，这样开发者最后调用EAGLContext的presentRenderBuffer方法，就可以将该渲染结果输出到屏幕上去了。

必须为每一个线程绑定OpenGL ES上下文。

### 首先创建上下文

```
EAGLContect* _context;
_context = [[EAGLConterxt alloc] initWithAPI:KEAGLRenderingAPIOpenGlES2];
```

### 然后实施绑定操作

```
[EAGLContext setCurrentContext:_context];
```

### 创建帧缓冲区

```
glGenFramebuffers(1, &_FrameBuffer);
```

### 创建绘制缓冲区

```
glGenRenderbuffers(1, &renderbuffer);
```

### 绑定帧缓冲区到渲染管线

```
glBindFramebuffer(GL_FRAMEBUFFER, _FrameBuffer);
```

### 绑定绘制缓冲区到渲染管线

```
glBindRenderbuffer(GL_RENDERBUFFER, _renderbuffer);
```

### 将CAEAGLLayer的绘制缓冲区作为绘制缓冲区的存储区

```
[_context renderbufferStorage:GL_RENDERBUFFER fromDrawable:(CAEAGLLayer*) self.layer]
```

### 获取绘制缓冲区的像素宽度

```
glGetRenderBufferParameteriv(GL_RENDER_BUFFER, GL_RENDER_BUFFER_WIDTH, &_backingWidth);
```

### 获取绘制缓冲区的像素高度

```
glGetRenderBufferParameteriv(GL_RENDER_BUFFER, GL_RENDER_BUFFER_HEIGHT, &_backingHeight);
```

### 将绘制缓冲区绑定 到帧缓冲区

```
glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_RENDERBUFFER, _renderbuffer)
```

### 检查FrameBuffer的status

```
GLenum status = glCheckFramebufferStatus(GL_FRAMEBUFFER); if(status != GL_FRAMEBUFFER_COMPLETE){ // failed to make complete frame buffer object }
```

### 调用绘制

```
[_context presentRenderbuffer:GL_RENDERBUFFER];
```

## OpenGL ES中的纹理

openGL中的纹理可以用来表示图像、照片、视频画面等数据，在视频渲染中，只需要处理二维的纹理，每个二维纹理都由许多小的纹理元素组成，类似于像素点。要使用纹理，最常用的方式是直接从一个图像文件加载数据。

### 创建一个纹理对象

```
void glGenTextures (GLsizei n, GLuint* textures)
```

### 调用OpenGL ES提供的一个绑定纹理对象的方法

```
glBindTexture(GL_TEXTURE_2D, texId);
```

执行完这行代码之后，下面的操作就都是针对于texID这个纹理对象的了。

### 设置纹理的过滤方式

> 当纹理对象（可以理解为一张图片）被渲染到物体表面上的时候（实际上是OpenGL绘制管线将纹理的元素映射到OpenGL生成的片段上的时候），有可能要被放大或者缩小，而当其放大或者缩小的时候，具体应该如何确定每个像素是如何被填充的，就由开发者配置的纹理对象的纹理过滤器来指明

+ 放大 GL_TEXTURE_MAG_FILTER
+ 缩小 GL_TEXTURE_MIN_FILTER
+ 双线性过滤 GL_LINEAR
+ 最邻近过滤 GL_NEAREST

### 将图片上传到纹理对象上

```
glTextImage2D(GL_TEXTURE_2D, 0,GL_RGBA, width, height,0,GL_RGBA,GL_UNSIGNED_BYTE,pixels);
//pixels 为图片的RGBA数据
```

## 利用自定义的shader，进行绘制

### 规定窗口的大小

```
glViewport(0, 0, screenWidth, screenHeight);
```

### 使用显卡绘制程序

```
glUseProgram（mGLProgId）;
```

### 设置物体坐标

```
GLfloat vertices[] = {-1.0f, -1.0f, 1.0f, -1.0f, -1.0f, 1.0f, 1.0f, 1.0f};
glVertexAttribPointer(mGLVertexCoords, 2, GL_FLOAT, 0, 0, vertices); glEnableVertexAttribArray(mGLVertexCoords);
```

### 设置纹理坐标

```
GLfloat texCoords1[] = { 0.0f, 0.0f, 1.0f, 0.0f, 0.0f, 1.0f, 1.0f, 1.0f }; GLfloat texCoords2[] = { 0.0f, 1.0f, 1.0f, 1.0f, 0.0f, 0.0f, 1.0f, 0.0f }; glVertexAttribPointer(mGLTextureCoords, 2, GL_FLOAT, 0, 0, texCoords2); glEnableVertexAttribArray(mGLTextureCoords);
```

### 指定将要绘制的纹理对象并且传递给对应的FragmentShader

```
glActiveTexture(GL_TEXTURE0); 
glBindTexture(GL_TEXTURE_2D, texId); 
glUniform1i(mGLUniformTexture, 0);
```

### 执行绘制操作

```
glDrawArrays(GL_TRIANGLE_STRIP, 0, 4);
```

